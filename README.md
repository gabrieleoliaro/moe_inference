# MoE inference

This repository contains our work on the design and implementation of a scalable inference system for MoE.

For now, we are familiarizing with existing solutions and benchmarking them. In particular, you can find scripts/instructions/containers/raw results for the following platforms:
- Fairseq by Meta: [benchmark/fairseq folder](./benchmark/fairseq)
- DeepSpeed-MoE by Microsoft: [benchmark/deepspeed_moe folder](./benchmark/deepspeed_moe)
- FasterTransformer by NVIDIA: [benchmark/faster_transformer folder](./benchmark/faster_transformer)

Please check the [docs](./docs) folder for additional info.