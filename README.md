# MoE inference

This repository contains our work on the design and implementation of a scalable inference system for MoE.

For now, we are familiarizing with existing solutions and benchmarking them. In particular, you can find scripts/instructions/containers/raw results for the following platforms:
- Fairseq by Meta: [benchmark/fairseq](./benchmark/fairseq) folder
- DeepSpeed-MoE by Microsoft: [benchmark/deepspeed_moe](./benchmark/deepspeed_moe) folder 
- FasterTransformer by NVIDIA: [benchmark/faster_transformer](./benchmark/faster_transformer) folder

Please check the [docs](./docs) folder for additional info.