# FasterTransformer MoE

TODO

## Parallelism in FasterTransformer

TODO: figure out what type of parallelism is used by FasterTransformer: data, model, pipeline, tensor parallelism?

## Synchronization/communication collectives

TODO: figure out what synchronization collectives are used. Eg: SPMD, gang-scheduling, MPMD, etc...?

## Implementing a MoE model in FasterTransformer

TODO: add support for MoE by writing our own implementation

## Pretrained MoE model

TODO: figure out whether we can reuse the [Fairseq pre-trained MoE model](https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm) or we need to obtain another one

## Benchmarking results

TODO